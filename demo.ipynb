{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b6c94f-3e79-46b7-b116-7027966777f8",
   "metadata": {},
   "source": [
    "# Mamba-2 Language Model demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa052505-d91c-4e87-8daa-2b00ad8cc881",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74ee1cb-b4b2-46a8-98a4-1dd845c1e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from mamba2 import Mamba2LMHeadModel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ab109-2cbe-4f7a-b5ce-a58b8860f98c",
   "metadata": {},
   "source": [
    "Official pretrained models on [huggingface](https://huggingface.co/state-spaces):\n",
    "* `state-spaces/mamba2-130m`\n",
    "* `state-spaces/mamba2-370m`\n",
    "* `state-spaces/mamba2-780m`\n",
    "* `state-spaces/mamba2-1.3b`\n",
    "* `state-spaces/mamba2-2.7b`\n",
    "\n",
    "Note that these are base models without fine-tuning for downstream tasks such as chat or instruction following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6569ffd-993f-4d5b-9094-902801fe6c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Choose a model depending on available system RAM (for Apple Silicon) or VRAM.\n",
    "model = Mamba2LMHeadModel.from_pretrained(\"state-spaces/mamba2-1.3b\", device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb837263-8a1f-40bf-a9b1-fce72225a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(\n",
    "    max_new_length=200,\n",
    "    temperature=1.0,\n",
    "    top_k=30,\n",
    "    top_p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87006a5d-7992-4026-9b40-36cbc3ebf8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, seed: int = 0, show_perf: bool = True):\n",
    "    \"\"\"Generate streaming completion\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)[0]\n",
    "    print(prompt, end=\"\")\n",
    "\n",
    "    start = time.process_time()\n",
    "    n_generated = 0\n",
    "    for i, (token_id, _hidden_state) in enumerate(model.generate(input_ids, **generation_config)):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        if i == 0:\n",
    "            now = time.process_time()\n",
    "            prompt_eval_elapsed, start = now - start, now\n",
    "        else:\n",
    "            n_generated += 1\n",
    "        print(token, end=\"\", flush=True)\n",
    "    if show_perf:\n",
    "        elapsed = time.process_time() - start\n",
    "        print('\\n\\n---')\n",
    "        print(f'Prompt eval | tokens: {input_ids.shape[0]} | elapsed: {prompt_eval_elapsed:.2f}s | tok/s: {input_ids.shape[0] / prompt_eval_elapsed:.2f}')\n",
    "        print(f'Generation | tokens: {n_generated} | elapsed: {elapsed:.2f}s | tok/s: {n_generated / elapsed:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b926b16-2883-4eef-9459-3718498409e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba is a new state space model architecture that enables the modeling of discrete events\n",
      "using the M-ary BDD model\n",
      "\n",
      "The Mamba architecture\n",
      "defines a state space model using a novel state abstraction that enables\n",
      "state to change and transition to a new state based on the values that change\n",
      "in any M-ary BDD node.\n",
      "\n",
      "Mamba’s state abstraction provides a unified representation of an\n",
      "event’s effects and effects of past events. When the M-ary BDD nodes\n",
      "change, each one of their corresponding states is represented with an\n",
      "indicator that changes when any state is changed from any other node to\n",
      "maintain a trace of changes. Mamba enables the modeling of\n",
      "events that can occur both synchronously or asynchronously.\n",
      "\n",
      "State transition is based on three types of\n",
      "events:\n",
      "\n",
      "Transition based on the change of any of the nodes of a M-ary BDD\n",
      "\n",
      "Transition based on an event being fired based on the change\n",
      "of any of the nodes\n",
      "\n",
      "---\n",
      "Prompt eval | tokens: 9 | elapsed: 1.41s | tok/s: 6.38\n",
      "Generation | tokens: 199 | elapsed: 12.76s | tok/s: 15.60\n"
     ]
    }
   ],
   "source": [
    "generate(\"Mamba is a new state space model architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608ccece-9a11-47bc-bafd-7b47fc6383c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is death\n",
      "\n",
      "The meaning of life is that people die and the living have to carry on\n",
      "\n",
      "The meaning of life is that if we don't do things today we have to do them tomorrow\n",
      "\n",
      "---\n",
      "Prompt eval | tokens: 5 | elapsed: 0.75s | tok/s: 6.66\n",
      "Generation | tokens: 38 | elapsed: 2.55s | tok/s: 14.90\n"
     ]
    }
   ],
   "source": [
    "generate(\"The meaning of life is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bc8a2b-b2bc-4d30-bf4c-213baec7441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is Nvidia's biggest moat on NVidia's biggest moat\n",
      "\n",
      "\"While CUDA is Nvidia's biggest moat for the time being, it's also clear that Nvidia has a huge opportunity to improve the performance and capabilities of its GPUs to keep pace with a new generation of processors based on ARM's big.LITTLE architecture.\"\n",
      "\n",
      "The problem with CUDA is that even if your game runs well on one of their products, the hardware is still far inferior to the competition. I can buy a Nvidia card that has better performance, but I can't do much to improve its design.\n",
      "\n",
      "When it comes to performance I'll stick to the Radeon 7990.\n",
      "\n",
      "I can also buy a GPU card, but I can't really do much to improve its capability other than buy more memory. While this might sound like a weak point for Nvidia, I would say that it's a strength for AMD, the market is there for both of them. If AMD wanted,\n",
      "\n",
      "---\n",
      "Prompt eval | tokens: 9 | elapsed: 0.73s | tok/s: 12.27\n",
      "Generation | tokens: 199 | elapsed: 12.72s | tok/s: 15.64\n"
     ]
    }
   ],
   "source": [
    "generate(\"CUDA is Nvidia's biggest moat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
